{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução à Dimensão VC e Dilema Bias-Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dimensão VC (Vapnik-Chervonenkis)\n",
    "\n",
    "A **Dimensão VC** é um conceito teórico que mede a capacidade de um modelo de aprendizado estatístico em fitar diferentes conjuntos de dados. Em outras palavras, ela representa o número de pontos que o modelo pode classificar sem erro, independentemente de sua distribuição. \n",
    "\n",
    "A Dimensão VC nos ajuda a entender quão flexível é um modelo. Um modelo com alta Dimensão VC pode ajustar-se a uma ampla variedade de formas e padrões nos dados. No entanto, é preciso ter cuidado, pois modelos com uma Dimensão VC muito alta podem facilmente se sobreajustar aos dados.\n",
    "\n",
    "---\n",
    "\n",
    "# Dilema Bias-Variance\n",
    "\n",
    "O **Dilema Bias-Variance** é um conceito fundamental em aprendizado de máquina que refere-se ao trade-off (Trade-off refere-se a uma situação em que, para ganhar uma coisa, você tem que sacrificar outra. Em outras palavras, é um equilíbrio entre dois fatores, onde melhorar um pode resultar na degradação do outro) que ocorre ao tentarmos minimizar os dois erros principais:\n",
    "\n",
    "\n",
    "# Trade-off entre Viés e Variância\n",
    "\n",
    "O equilíbrio entre viés e variância é um conceito central no aprendizado de máquina, e entender essa relação é crucial para construir modelos robustos e precisos. Vamos entender a importância destes termos:\n",
    "\n",
    "1. **Importância do Bias (Viés):** \n",
    "O viés nos dá uma indicação de quão longe nossas previsões estão dos valores reais. Se o viés é alto, significa que nosso modelo está fazendo suposições erradas sobre a natureza da relação entre as entradas e as saídas. Ignorar o viés elevado pode levar a modelos que sistematicamente erram as previsões, independentemente da quantidade de dados disponível.\n",
    "\n",
    "2. **Importância da Variance (Variância):** \n",
    "A variância nos dá uma indicação da sensibilidade do modelo às flutuações nos dados. Um modelo com alta variância se ajusta muito aos dados de treinamento, mas pode falhar ao generalizar para novos dados. Se ignorarmos a variância, podemos criar modelos que se ajustam perfeitamente aos dados de treinamento, mas que falham miseravelmente em qualquer dado novo.\n",
    "\n",
    "**Por que é crucial entender esses conceitos?** \n",
    "Simplificando, porque queremos modelos que generalizem bem para dados não vistos. Se focarmos apenas em reduzir o viés (fazendo o modelo se ajustar perfeitamente aos dados de treinamento), podemos aumentar a variância, o que reduz a capacidade do modelo de se generalizar. Por outro lado, se focarmos apenas em reduzir a variância (usando modelos muito simples), podemos não capturar todas as informações relevantes nos dados, resultando em um alto viés. O equilíbrio entre viés e variância nos permite construir modelos que são simultaneamente precisos e generalizáveis.\n",
    "\n",
    "Em termos simples, **Bias** é o erro de nosso modelo devido à suposição simplista. Enquanto **Variance** é o erro devido ao modelo ser excessivamente complexo.\n",
    "\n",
    "O objetivo em qualquer problema de aprendizado de máquina é encontrar um equilíbrio entre o viés e a variância, de modo que o erro total seja mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensão VC (Vapnik-Chervonenkis)\n",
    "\n",
    "## Definição e Origem\n",
    "\n",
    "A **Dimensão VC**, nomeada em homenagem aos matemáticos Vladimir Vapnik e Alexey Chervonenkis, é uma medida teórica usada para quantificar a capacidade de um modelo de aprendizado estatístico de fitar dados. De maneira simples, é uma métrica que nos ajuda a entender o quão flexível um modelo pode ser.\n",
    "\n",
    "## Capacidade do Modelo\n",
    "\n",
    "Um modelo com uma **alta Dimensão VC** tem uma capacidade elevada de se ajustar perfeitamente a um conjunto de dados, mesmo que este conjunto seja muito ruidoso ou contenha outliers. Por outro lado, um modelo com **baixa Dimensão VC** pode não conseguir se ajustar a conjuntos de dados mais complexos, mas tem a vantagem de ser menos suscetível ao overfitting.\n",
    "\n",
    "## Relação com a Complexidade do Modelo\n",
    "\n",
    "A Dimensão VC está intrinsecamente ligada à complexidade do modelo:\n",
    "- Modelos mais **simples** tendem a ter uma **Dimensão VC baixa**.\n",
    "- Modelos mais **complexos**, como redes neurais profundas, podem ter uma **Dimensão VC muito alta**.\n",
    "\n",
    "No entanto, é crucial entender que uma alta Dimensão VC não garante que o modelo terá um bom desempenho em dados não vistos. Precisamos equilibrar essa capacidade do modelo com a quantidade e qualidade dos dados disponíveis e com o risco de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com Polinômios\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Gerando dados para o exemplo\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - np.random.rand(16))\n",
    "\n",
    "# Função para plotar os resultados\n",
    "def plot_results(model, title, subplot):\n",
    "    model.fit(X, y)\n",
    "    ax = plt.subplot(subplot)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    X_test = np.linspace(0, 5, 100)[:, np.newaxis]\n",
    "    plt.plot(X_test, model.predict(X_test), label=\"Modelo\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Amostras\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "# Modelos com diferentes graus de polinômio (representando diferentes Dimensões VC)\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "for i, degree in enumerate(degrees, start=1):\n",
    "    polynomial = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    model = Pipeline([(\"polynomial_features\", polynomial), (\"linear_regression\", linear_regression)])\n",
    "    plot_results(model, f\"Polinômio de grau {degree}\", 131 + i - 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comentários:\n",
    "# O modelo de grau 1 (Dimensão VC baixa) não se ajusta bem aos dados.\n",
    "# O modelo de grau 4 tem um bom ajuste.\n",
    "# O modelo de grau 15 (Dimensão VC alta) se ajusta perfeitamente às amostras, incluindo o ruído, indicando overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com redes neurais\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Função para plotar os resultados\n",
    "def plot_results_nn(model, title, subplot):\n",
    "    model.fit(X, y, epochs=1000, verbose=0)\n",
    "    ax = plt.subplot(subplot)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    X_test = np.linspace(0, 5, 100)[:, np.newaxis]\n",
    "    plt.plot(X_test, model.predict(X_test), label=\"Modelo\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Amostras\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "# Modelo que aproxima o Polinômio de Grau 1\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(1, activation='linear', input_dim=1))  # Camada de entrada e única camada oculta\n",
    "model1.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Modelo que aproxima o Polinômio de Grau 4\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(5, activation='relu', input_dim=1))  # Camada de entrada\n",
    "model4.add(Dense(5, activation='relu'))  # Primeira camada oculta\n",
    "model4.add(Dense(1, activation='linear'))  # Camada de saída\n",
    "model4.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Modelo que aproxima o Polinômio de Grau 15\n",
    "model15 = Sequential()\n",
    "model15.add(Dense(50, activation='relu', input_dim=1))  # Camada de entrada\n",
    "model15.add(Dense(30, activation='relu'))  # Primeira camada oculta\n",
    "model15.add(Dense(30, activation='relu'))  # Primeira camada oculta\n",
    "model15.add(Dense(1, activation='linear'))  # Camada de saída\n",
    "model15.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Plotando os resultados\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plot_results_nn(model1, \"Rede Neural - Aproximação Grau 1\", 131)\n",
    "plot_results_nn(model4, \"Rede Neural - Aproximação Grau 4\", 132)\n",
    "plot_results_nn(model15, \"Rede Neural - Aproximação Grau 15\", 133)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Carregando os dados\n",
    "df = pd.read_csv('notebooks.csv')\n",
    "X = df.drop(columns='valor').values\n",
    "y = df['valor'].values\n",
    "\n",
    "# Dividindo os dados em conjuntos de teste (10%) e o restante (90%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dividindo o X_temp em três conjuntos de treinamento de tamanhos iguais\n",
    "X_train1, X_temp, y_train1, y_temp = train_test_split(X_temp, y_temp, test_size=(2/3), random_state=42)\n",
    "X_train2, X_train3, y_train2, y_train3 = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_trains = [X_train1, X_train2, X_train3]\n",
    "y_trains = [y_train1, y_train2, y_train3]\n",
    "\n",
    "# Treinando e avaliando modelos\n",
    "for i, (X_t, y_t) in enumerate(zip(X_trains, y_trains)):\n",
    "    # Usamos um modelo polinomial de grau 2 como exemplo\n",
    "    model = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "                      (\"linear_regression\", LinearRegression())])\n",
    "    model.fit(X_t, y_t)\n",
    "    \n",
    "    train_error = mean_squared_error(y_t, model.predict(X_t))\n",
    "    test_error = mean_squared_error(y_test, model.predict(X_test))\n",
    "    \n",
    "    print(f\"Subconjunto de treinamento {i+1} (Tamanho {len(X_t)}):\")\n",
    "    print(f\"Erro de treinamento: {train_error:.2f}\")\n",
    "    print(f\"Erro de teste: {test_error:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Resultados e Relação com a Dimensão VC\n",
    "\n",
    "## Consistência do Erro\n",
    "\n",
    "- **Visão Geral**: A consistência nos erros de treinamento e teste em todos os subconjuntos indica que a capacidade do modelo (ou sua Dimensão VC) está relativamente bem alinhada com a complexidade dos dados.\n",
    "  \n",
    "- **Sinal de Alta Capacidade**: Se tivéssemos um modelo de alta capacidade (alta Dimensão VC), poderíamos ver um erro de treinamento muito baixo, mas um erro de teste significativamente mais alto, indicando overfitting.\n",
    "\n",
    "## Variação entre Subconjuntos\n",
    "\n",
    "- **Influência dos Dados no Treinamento**: A variação nos erros entre diferentes subconjuntos reflete a influência dos dados específicos no treinamento. \n",
    "  \n",
    "- **Análise da Variação**: Essa variação não é drasticamente diferente, o que indica que a capacidade do modelo é suficientemente flexível para se adaptar a diferentes subconjuntos de dados.\n",
    "  \n",
    "- **Sinal de Baixa Capacidade**: Se o modelo tivesse uma Dimensão VC muito baixa (pouca capacidade), poderíamos ver uma grande variação nos erros de treinamento/teste, porque o modelo não seria capaz de se ajustar bem a variações nos dados.\n",
    "\n",
    "## Relação com a Dimensão VC\n",
    "\n",
    "- **Estimativa da Dimensão VC**: Para ter uma noção exata da Dimensão VC, teríamos que avaliar o modelo em todos os possíveis subconjuntos de dados e ver como ele se comporta. Na prática, isso é inviável, então usamos métricas como erro de treinamento e teste para ter uma ideia.\n",
    "  \n",
    "- **Analisando os Erros**: \n",
    "  - **Overfitting**: Se os erros de treinamento forem consistentemente baixos e os erros de teste consistentemente altos em vários subconjuntos, isso pode indicar um modelo com uma Dimensão VC muito alta.\n",
    "  - **Underfitting**: Se ambos os erros forem altos, o modelo pode ter uma Dimensão VC muito baixa.\n",
    "\n",
    "- **Conclusão para o Caso Atual**: Em nosso caso, o modelo parece ter uma capacidade adequada para os dados, pois os erros de treinamento e teste são relativamente consistentes em diferentes subconjuntos. No entanto, para ter uma compreensão mais clara da Dimensão VC, seria ideal experimentar com modelos de diferentes complexidades e observar como os erros de treinamento e teste variam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilema Bias-Variance\n",
    "\n",
    "## Introdução ao Bias e Variance\n",
    "\n",
    "Ao treinar modelos de machine learning, frequentemente nos deparamos com duas fontes principais de erros: **Bias (Viés)** e **Variance (Variância)**.\n",
    "\n",
    "- **Bias (Viés)**: Erro devido a suposições errôneas no algoritmo de aprendizado. Um alto bias pode fazer o algoritmo perder relações relevantes entre características e saídas previstas, resultando em um modelo subajustado (underfitting).\n",
    "- **Variance (Variância)**: Erro devido à sensibilidade excessiva às pequenas flutuações no conjunto de treinamento. Uma alta variância pode fazer com que um modelo capture ruído nos dados de treinamento, levando ao sobreajuste (overfitting).\n",
    "\n",
    "## Impacto na Performance do Modelo\n",
    "\n",
    "- Modelos com **alto bias** não são flexíveis o suficiente para capturar padrões complexos nos dados.\n",
    "- Modelos com **alta variância** capturam demais os dados, incluindo o ruído, e não generalizam bem para novos dados.\n",
    "\n",
    "## Decomposição do Erro\n",
    "\n",
    "O erro de um modelo pode ser decomposto da seguinte forma:\n",
    "\\[ Erro \\ Total = Viés^2 + Variância + Erro \\ Irredutível \\]\n",
    "\n",
    "Onde:\n",
    "- **Erro Irredutível**: É o erro introduzido por fatores desconhecidos ou inerentes ao problema. É uma constante e independe do modelo escolhido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo, visualizaremos o efeito do bias e da variância ao ajustar a complexidade de um modelo e entenderemos sua relação com o erro total\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gerando dados para o exemplo\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - np.random.rand(16))\n",
    "\n",
    "# Função para calcular bias, variance e erro total\n",
    "def calculate_bias_variance(models, X_train, X_test, y_train, y_test):\n",
    "    biases, variances, errors = [], [], []\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculando bias e variance\n",
    "        bias = np.mean((predictions - y_test) ** 2)\n",
    "        variance = np.var(predictions)\n",
    "        error = mean_squared_error(y_test, predictions)\n",
    "        \n",
    "        biases.append(bias)\n",
    "        variances.append(variance)\n",
    "        errors.append(error)\n",
    "    return biases, variances, errors\n",
    "\n",
    "# Dividindo o conjunto de dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criando modelos com diferentes graus de polinômio\n",
    "degrees = list(range(1, 10))\n",
    "models = [Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "                    (\"linear_regression\", LinearRegression())]) for degree in degrees]\n",
    "\n",
    "biases, variances, errors = calculate_bias_variance(models, X_train, X_test, y_train, y_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Criando um eixo para bias\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(range(1, 10), biases, label='Bias ($Bias^2$)', marker='o', color='blue')\n",
    "ax1.set_xlabel('Complexidade do Modelo (10x neurônios)')\n",
    "ax1.set_ylabel('Bias ($Bias^2$)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Criando um segundo eixo para variância e erro total\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1, 10), variances, label='Variance', marker='o', color='green')\n",
    "ax2.plot(range(1, 10), errors, label='Erro Total', marker='o', color='red')\n",
    "ax2.set_ylabel('Variance / Erro Total', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.title('Dilema Bias-Variance com Diferentes Graus de Polinômios')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# A medida que o grau do polinômio aumenta, o bias diminui, mas a variância aumenta.\n",
    "# O erro total tem um ponto de mínimo, onde a combinação de bias e variância é ótima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Função para calcular bias, variance e erro total\n",
    "def calculate_bias_variance(models, X_train, X_test, y_train, y_test):\n",
    "    biases, variances, errors = [], [], []\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train, epochs=150, verbose=0)  # Treinando cada modelo\n",
    "        predictions = model.predict(X_test).flatten()  # Previsões do modelo\n",
    "        \n",
    "        # Calculando bias, variance e erro total\n",
    "        bias = np.mean((predictions - y_test) ** 2)\n",
    "        variance = np.var(predictions)\n",
    "        error = mean_squared_error(y_test, predictions)\n",
    "        \n",
    "        biases.append(bias)\n",
    "        variances.append(variance)\n",
    "        errors.append(error)\n",
    "    return biases, variances, errors\n",
    "\n",
    "# Dividindo o conjunto de dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criando modelos de redes neurais correspondentes a diferentes \"complexidades\"\n",
    "models = []\n",
    "for cont in range(1, 10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10*cont , activation='relu', input_dim=1))  # Camada de entrada e oculta, aumentando neurônios conforme o loop\n",
    "    model.add(Dense(1, activation='linear'))  # Camada de saída\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    models.append(model)\n",
    "\n",
    "biases, variances, errors = calculate_bias_variance(models, X_train, X_test, y_train, y_test)\n",
    "clear_output(wait=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Criando um eixo para bias\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(range(1, 10), biases, label='Bias ($Bias^2$)', marker='o', color='blue')\n",
    "ax1.set_xlabel('Complexidade do Modelo (10x neurônios)')\n",
    "ax1.set_ylabel('Bias ($Bias^2$)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Criando um segundo eixo para variância e erro total\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1, 10), variances, label='Variance', marker='o', color='green')\n",
    "ax2.plot(range(1, 10), errors, label='Erro Total', marker='o', color='red')\n",
    "ax2.set_ylabel('Variance / Erro Total', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.title('Dilema Bias-Variance com Redes Neurais de Diferentes Complexidades')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# À medida que a complexidade do modelo (número de neurônios) aumenta, o bias tende a diminuir, mas a variância aumenta.\n",
    "# O erro total tem um ponto de mínimo, onde a combinação de bias e variância é ótima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Demonstração Prática: Overfitting e Underfitting\n",
    "\n",
    "## Relembrando conceitos\n",
    "\n",
    "**Overfitting**:\n",
    "- Quando o modelo é muito complexo e se ajusta demais aos dados de treinamento, mas tem um desempenho ruim em novos dados não vistos.\n",
    "- Captura ruídos e anomalias dos dados, em vez de padrões gerais.\n",
    "\n",
    "**Underfitting**:\n",
    "- Ocorre quando o modelo é muito simples para capturar as nuances e padrões dos dados.\n",
    "- Pode falhar tanto nos dados de treinamento quanto nos dados não vistos.\n",
    "\n",
    "---\n",
    "\n",
    "## Relação com o Dilema Bias-Variance\n",
    "\n",
    "- **Overfitting** está associado a uma **baixa bias** (pois ajusta-se bem aos dados de treinamento) e **alta variância** (pois varia muito com diferentes conjuntos de treinamento).\n",
    "- **Underfitting** está relacionado com **alto bias** (pois não se ajusta bem mesmo aos dados de treinamento) e **baixa variância** (pois a resposta não muda muito com diferentes conjuntos de treinamento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('casas_boston.csv')\n",
    "\n",
    "# Separando a coluna de quartos e valor (target)\n",
    "X = df[['quartos']]\n",
    "y = df['valor']\n",
    "# Dividindo os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Função para treinar e visualizar o modelo\n",
    "def train_and_plot(degree=1):\n",
    "    # Transformando o feature com PolynomialFeatures\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    # Treinando o modelo de regressão\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predições\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    # Métricas de erro\n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Plotando\n",
    "    plt.scatter(X_train, y_train, color='blue', label='Treino')\n",
    "    plt.scatter(X_test, y_test, color='red', label='Teste')\n",
    "    \n",
    "    X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "    X_range_poly = poly.transform(X_range)\n",
    "    y_range_pred = model.predict(X_range_poly)\n",
    "    \n",
    "    plt.plot(X_range, y_range_pred, color='green', label=f'Polinômio Grau {degree}')\n",
    "    plt.title(f'Overfitting e Underfitting (Grau {degree})\\nErro de Treino: {train_error:.2f} | Erro de Teste: {test_error:.2f}')\n",
    "    plt.xlabel('Número Médio de Quartos por Moradia')\n",
    "    plt.ylabel('Preço da Moradia (em $1000s)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_error, test_error\n",
    "\n",
    "# Modelo de Regressão Linear Simples (Underfitting)\n",
    "train_and_plot(1)\n",
    "\n",
    "# Modelo de Regressão Linear Simples (Underfitting)\n",
    "train_and_plot(3)\n",
    "\n",
    "# Modelo Polinomial de Grau 10 (Overfitting)\n",
    "train_and_plot(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
